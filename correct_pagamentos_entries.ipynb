{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data from the CSV files\n",
    "base_info = pd.read_csv('base_info.csv')\n",
    "base_pagamentos = pd.read_csv('base_pagamentos_desenvolvimento.csv')\n",
    "base_cadastral = pd.read_csv('base_cadastral.csv')\n",
    "\n",
    "# lower case all column names\n",
    "base_info.columns = base_info.columns.str.lower()\n",
    "base_pagamentos.columns = base_pagamentos.columns.str.lower()\n",
    "base_cadastral.columns = base_cadastral.columns.str.lower()\n",
    "\n",
    "# give a new id to each client based on registrartion date order\n",
    "base_cadastral = base_cadastral.sort_values('data_cadastro').reset_index(drop=True)\n",
    "base_cadastral['new_id'] = range(1, len(base_cadastral) + 1)\n",
    "\n",
    "# save a corresponding dictionary of old_id and new_id\n",
    "id_dict = base_cadastral[['id_cliente', 'new_id']].set_index('id_cliente').to_dict()\n",
    "\n",
    "# convert str to date\n",
    "def convert_date(df, to_date_columns, format):\n",
    "  df_copy = df.copy()\n",
    "  for col in to_date_columns:\n",
    "    df_copy[col] = pd.to_datetime(df_copy[col], format=format)#.dt.date\n",
    "  return df_copy\n",
    "\n",
    "date_columns = ['data_emissao_documento', 'data_pagamento', 'data_vencimento']\n",
    "base_pagamentos_date = convert_date(base_pagamentos, date_columns, '%Y-%m-%d')\n",
    "base_pagamentos_date = convert_date(base_pagamentos_date, ['safra_ref'], '%Y-%m')\n",
    "base_info_date = convert_date(base_info, ['safra_ref'], '%Y-%m')\n",
    "base_cadastral_date = convert_date(base_cadastral, ['data_cadastro'], '%Y-%m-%d')\n",
    "\n",
    "# ajust values in base_cadastral\n",
    "base_cadastral_date.flag_pf = (base_cadastral_date.flag_pf == 'X').astype(int)\n",
    "base_cadastral_date.segmento_industrial = base_cadastral_date.segmento_industrial.fillna('NAN')\n",
    "base_cadastral_date.dominio_email = base_cadastral_date.dominio_email.fillna('NAN')\n",
    "base_cadastral_date.porte = base_cadastral_date.porte.fillna('NAN')\n",
    "base_cadastral_date.cep_2_dig = base_cadastral_date.cep_2_dig.fillna('NA')\n",
    "base_cadastral_date.ddd = base_cadastral_date.ddd.fillna('-1')\n",
    "base_cadastral_date.loc[base_cadastral_date['ddd'].str.contains(\"\\(\"), 'ddd'] = '-2'\n",
    "\n",
    "# Add a binary column, fraud, to base_pagamentos_desenvolvimento_coherent and assing 1 to rows where DATA_PAGAMENTO > DATA_VENCIMENTO + 5\n",
    "base_pagamentos_date['late_payment'] = (base_pagamentos_date['data_pagamento'] - base_pagamentos_date['data_vencimento']).dt.days\n",
    "base_pagamentos_date['fraud'] = np.where(base_pagamentos_date['late_payment'] > 5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## base_cadastral encodings\n",
    "\n",
    "# date encoding\n",
    "def encode_date(df, columns):\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        min_date = df_copy[col].min()\n",
    "        df_copy[col + '_since_min'] = (df_copy[col] - min_date).dt.days + 1\n",
    "        # df_copy[col + '_year'] = df_copy[col].dt.year\n",
    "        # df_copy[col + '_month'] = df_copy[col].dt.month\n",
    "        # df_copy[col + '_day'] = df_copy[col].dt.day\n",
    "        # df_copy[col + '_dayofweek'] = df_copy[col].dt.dayofweek\n",
    "        # df_copy[col + '_dayofyear'] = df_copy[col].dt.dayofyear\n",
    "        # df_copy[col + '_weekofyear'] = df_copy[col].dt.weekofyear\n",
    "        # df_copy[col + '_quarter'] = df_copy[col].dt.quarter\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "def encode_categorical(df, columns):\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        df_copy = pd.concat([df_copy, pd.get_dummies(df_copy[col], prefix=col)], axis=1)\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "# ordinal encoding for categorical variables\n",
    "def encode_ordinal(df, columns, mapping):\n",
    "    df_copy = df.copy()\n",
    "    for i, col in enumerate(columns):\n",
    "        df_copy[col + '_encoded'] = df_copy[col].map(mapping[i])\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "ordinal_encode_map = [{'NAN': 0, 'PEQUENO': 1, 'MEDIO': 2, 'GRANDE': 3}]\n",
    "base_cadastral_date_encoded = encode_date(base_cadastral_date, ['data_cadastro'])\n",
    "base_cadastral_one_hot_encoded = encode_categorical(base_cadastral_date_encoded, ['segmento_industrial', 'dominio_email'])\n",
    "base_cadastral_ordinal_encoded = encode_ordinal(base_cadastral_one_hot_encoded, ['porte'], ordinal_encode_map)\n",
    "base_cadastral_droped = base_cadastral_ordinal_encoded.drop(['id_cliente', 'ddd', 'cep_2_dig'], axis=1)\n",
    "base_cadastral_droped\n",
    "\n",
    "# standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "base_cadastral_scaled = scaler.fit_transform(base_cadastral_droped)\n",
    "base_cadastral_scaled = pd.DataFrame(base_cadastral_scaled, columns=base_cadastral_droped.columns)\n",
    "display(base_cadastral_scaled.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "base_cadastral_tsne = TSNE(n_components=3).fit_transform(base_cadastral_scaled)\n",
    "print(base_cadastral_tsne.shape)\n",
    "\n",
    "# plot 16 TSNEs with different columns as color\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(base_cadastral_scaled.columns):\n",
    "    ax = fig.add_subplot(4, 4, i+1, projection='3d')\n",
    "    ax.scatter(base_cadastral_tsne[:, 0], base_cadastral_tsne[:, 1], base_cadastral_tsne[:, 2], c=base_cadastral_scaled[col], cmap='coolwarm')\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cadastral_tsne_df = pd.DataFrame(base_cadastral_tsne, columns=['tsne_1', 'tsne_2', 'tsne_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "input_dim = base_cadastral_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "# make encoder\n",
    "encoder0 = Dense(25, activation='relu')(input_layer)\n",
    "encoder0 = BatchNormalization()(encoder0)\n",
    "encoder0 = Dropout(0.1)(encoder0)\n",
    "encoder0 = Dense(15, activation='relu')(encoder0)\n",
    "encoder0 = BatchNormalization()(encoder0)\n",
    "encoder0 = Dropout(0.1)(encoder0)\n",
    "encoder0 = Dense(15, activation='relu')(encoder0 + input_layer)\n",
    "encoder1 = BatchNormalization()(encoder0)\n",
    "encoder1 = Dropout(0.1)(encoder1)\n",
    "encoder1 = Dense(15, activation='relu')(encoder1)\n",
    "encoder1 = BatchNormalization()(encoder1)\n",
    "encoder1 = Dropout(0.1)(encoder1)\n",
    "encoder1 = Dense(15, activation='relu')(encoder1 + encoder0)\n",
    "encoder1 = BatchNormalization()(encoder1)\n",
    "encoder1 = Dropout(0.1)(encoder1)\n",
    "encoder = Dense(encoding_dim, activation='linear')(encoder1)\n",
    "# make decoder\n",
    "decoder0 = Dense(15, activation='relu')(encoder)\n",
    "decoder1 = BatchNormalization()(decoder0)\n",
    "decoder1 = Dropout(0.1)(decoder1)\n",
    "decoder1 = Dense(15, activation='relu')(decoder1)\n",
    "decoder1 = BatchNormalization()(decoder1)\n",
    "decoder1 = Dropout(0.1)(decoder1)\n",
    "decoder = Dense(15, activation='relu')(decoder1 + decoder0)\n",
    "decoder = BatchNormalization()(decoder)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = Dense(15, activation='relu')(decoder)\n",
    "decoder = BatchNormalization()(decoder)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = Dense(25, activation='relu')(decoder)\n",
    "decoder = BatchNormalization()(decoder)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = Dense(input_dim, activation='linear')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "adam = Adam(learning_rate=0.002, weight_decay=0.0001)\n",
    "autoencoder.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "history = autoencoder.fit(base_cadastral_scaled, base_cadastral_scaled, epochs=200, batch_size=64, shuffle=True, validation_split=0.2)\n",
    "\n",
    "encoder = Model(inputs=input_layer, outputs=encoder)\n",
    "encoded_data = encoder.predict(base_cadastral_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss: 0.3845 - accuracy: 0.8669 - val_loss: 0.5378 - val_accuracy: 0.7452\n",
    "# loss: 0.3414 - accuracy: 0.7405 - val_loss: 0.4845 - val_accuracy: 0.8745\n",
    "# loss: 0.3297 - accuracy: 0.7357 - val_loss: 0.4748 - val_accuracy: 0.8745\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_df = pd.DataFrame(encoded_data, columns=['encoded_1', 'encoded_2', 'encoded_3'])\n",
    "encoded_data_df['segmento_industrial'] = base_cadastral['segmento_industrial']\n",
    "encoded_data_df['porte'] = base_cadastral['porte']\n",
    "encoded_data_df['dominio_email'] = base_cadastral['dominio_email']\n",
    "encoded_data_df['data_cadastro_since_min'] = base_cadastral_scaled['data_cadastro_since_min']\n",
    "encoded_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check uniqueness of each encoded representation\n",
    "encoded_data_df.groupby(['encoded_1', 'encoded_2', 'encoded_3']).size().reset_index().rename(columns={0:'count'}).sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the encoded data is linearly separable unsing 16 plots\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(base_cadastral_scaled.columns):\n",
    "    ax = fig.add_subplot(4, 4, i+1, projection='3d')\n",
    "    ax.scatter(encoded_data_df['encoded_1'], encoded_data_df['encoded_2'], encoded_data_df['encoded_3'], c=base_cadastral_scaled[col], cmap='coolwarm')\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = fig.add_subplot(4, 4, i+1)\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:  encoder 15, decoder 15, sigmoid\n",
    "# 2:  encoder 15, decoder 15, linear\n",
    "# 3:  encoder 10, 10, 3, decoder 10, 10, 15, sigmoid\n",
    "# 4:  encoder 10, dropout, 10, dropout, 3, decoder 10, dropout, 10, dropout, 15, sigmoid\n",
    "# 5:  encoder 10, dropout, 10, dropout, 3, decoder 10, dropout, 10, dropout, 15, linear\n",
    "# 6:  encoder 15, dropout, 10, dropout, 10, dropout, 3, decoder 10, dropout, 10, dropout, 10, dropout, 15, linear, bach_size=64 (double)\n",
    "# 7:  encoder 15, BN, dropout, 10, BN, dropout, 10, BN, dropout, 3, decoder 10, BN, dropout, 10, BN, dropout, 10, BN, dropout, 15, linear | 17: like 7 w_16 perks, but with BN an linear f. encoder yielded zero repeated representations, v_loss=0.58\n",
    "# 8:  encoder 25, BN, dropout, 15, BN, dropout, 15, BN, dropout, 3, decoder 15, BN, dropout, 15, BN, dropout, 25, BN, dropout, 15, linear\n",
    "# 9:  encoder 25, BN, dropout, 15, BN, dropout, 15, BN, dropout, 3, decoder 15, BN, dropout, 15, BN, dropout, 25, BN, dropout, 15, linear\n",
    "# 10: encoder 25, BN, dropout, 15, BN, dropout, 15, BN, dropout, 3, decoder 15, BN, dropout, 15, BN, dropout, 25, BN, dropout, 15, linear (1/0 skip enc/dec) (double trainning)\n",
    "# 11: enc.b 25, enc.b 15, enc.b 15, enc.b 15, enc.b 15, 3, dec.b 15, dec.b 15, dec.b 15, dec.b 25, 15, linear (2/1 skip enc/dec)\n",
    "# 12: enc.b 25, enc.b 15, enc.b 15, enc.b 15, enc.b 15, 3, dec.b 15, dec.b 15, dec.b 15, dec.b 25, 15, linear (2/1 skip enc/dec) (double lr)\n",
    "# 13: enc.b 25, enc.b 15, enc.b 15, enc.b 15, enc.b 15, 3, dec.b 15, dec.b 15, dec.b 15, dec.b 25, 15, linear (2/1 skip enc/dec) (L2 0.0001)\n",
    "# 14: enc.b 25, enc.b 15, enc.b 15, enc.b 15, enc.b 15, 3, dec.b 15, dec.b 15, dec.b 15, dec.b 25, 15, linear (2/1 skip enc/dec) ???\n",
    "# 15: enc.b 25, enc.b 25, enc.b 25, enc.b 25, enc.b 15, 3, dec.b 15, dec.b 25, dec.b 25, dec.b 25, 15, linear (2/1 skip enc/dec) (double depth, \"double\" units)\n",
    "# 16: enc.b 25, enc.b 15, enc.b 15, enc.b 15, enc.b 15, 3, dec.b 15, dec.b 15, dec.b 15, dec.b 25, 15, linear (2/1 skip enc/dec) (13 but with linear act.fun. for last layer in the encoder) ==> yeilded 11 repeated representations\n",
    "\n",
    "# # save the collection of histories to a file\n",
    "# import pickle\n",
    "# with open('histories.pickle', 'wb') as f:\n",
    "#     pickle.dump(histories, f)\n",
    "\n",
    "# # load the collection of histories from a file\n",
    "# import pickle\n",
    "# with open('histories.pickle', 'rb') as f:\n",
    "#     histories_loaded = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def create_layers(input_layer, units, activation='relu', dropout_rate=0.1):\n",
    "    x_dense = Dense(units, activation=activation)(input_layer)\n",
    "    x_bn = BatchNormalization()(x_dense)\n",
    "    x_drop = Dropout(dropout_rate)(x_bn)\n",
    "    return [x_dense, x_bn, x_drop]\n",
    "\n",
    "def autoencoder_model(input_dim, encoding_dim, learning_rate=0.002, weight_decay=0.0001):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    # encoder\n",
    "    encoder = create_layers(input_layer, 25)\n",
    "    encoder = create_layers(encoder[2], 15)\n",
    "    encoder = create_layers(encoder[2] + input_layer, 15)\n",
    "    encoder_skip = create_layers(encoder[2], 15)\n",
    "    encoder = create_layers(encoder_skip[2] + encoder[1], 15)\n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoder[2])\n",
    "\n",
    "    # decoder\n",
    "    decoder = create_layers(encoded, 15)\n",
    "    decoder_skip = create_layers(decoder[2], 15)\n",
    "    decoder = create_layers(decoder_skip[2] + decoder[1], 15)\n",
    "    decoder = create_layers(decoder[2], 15)\n",
    "    decoder = create_layers(decoder[2], 25)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoder[2])\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    autoencoder.compile(optimizer=adam, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "input_dim = base_cadastral_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "\n",
    "autoencoder = autoencoder_model(input_dim, encoding_dim)\n",
    "history = autoencoder.fit(base_cadastral_scaled, base_cadastral_scaled, epochs=200, batch_size=64, shuffle=True, validation_split=0.2)\n",
    "\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-18].output)\n",
    "encoded_data = encoder.predict(base_cadastral_scaled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
