{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "base_cadastral_pca = PCA(n_components=3).fit_transform(base_cadastral_scaled)\n",
    "print(base_cadastral_pca.shape)\n",
    "\n",
    "# use UMAP to reduce dimensionality\n",
    "import umap\n",
    "base_cadastral_umap = umap.UMAP(n_components=3).fit_transform(base_cadastral_scaled)\n",
    "print(base_cadastral_umap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the PCA\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(base_cadastral_pca[:, 0], base_cadastral_pca[:, 1], base_cadastral_pca[:, 2], c=c_column, cmap='coolwarm')\n",
    "plt.title('PCA')\n",
    "plt.show()\n",
    "\n",
    "# plot the UMAP\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(base_cadastral_umap[:, 0], base_cadastral_umap[:, 1], base_cadastral_umap[:, 2], c=c_column, cmap='coolwarm')\n",
    "plt.title('UMAP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that ddd and cep_2_dig are not good features using chi2 test\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "X = base_cadastral_scaled.drop(['fraud'], axis=1)\n",
    "y = base_cadastral_scaled['fraud']  \n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "print(X_kbest.shape)\n",
    "print(chi2_selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fraud_dependent_of_the_data_exclusive_to_pagamentos_table(info, pagamentos):\n",
    "  info = info.copy()\n",
    "  pagamentos = pagamentos.copy()\n",
    "\n",
    "  # lable as 1 all rows of pagamentos where the pair (id_cliente, safra_ref) is in info and as 0 otherwise\n",
    "  pagamentos['id_cliente_safra_ref'] = pagamentos['id_cliente'].astype(str) + '_' + pagamentos['safra_ref'].astype(str)\n",
    "  info['id_cliente_safra_ref'] = info['id_cliente'].astype(str) + '_' + info['safra_ref'].astype(str)\n",
    "  pagamentos['coherent'] = np.where(pagamentos['id_cliente_safra_ref'].isin(info['id_cliente_safra_ref']), 1, 0)\n",
    "\n",
    "  # Chi-square test to determine if the fraud is independent of the coherent column\n",
    "  from scipy.stats import chi2_contingency\n",
    "  contingency_table = pd.crosstab(pagamentos['fraud'], pagamentos['coherent'])\n",
    "  stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "  if p < 0.05:\n",
    "    print('The fraud is dependent of the data in pagamentos where its key does not appear in the info table.\\n' +\n",
    "          'This data should be kept in the pagamentos table')\n",
    "  else:\n",
    "    print('The fraud is independent of the data in pagamentos where its key does not appear in the info table\\n' +\n",
    "          'This data can be removed from the pagamentos table')\n",
    "  return p < 0.05\n",
    "\n",
    "if not is_fraud_dependent_of_the_data_exclusive_to_pagamentos_table(base_info_date, base_pagamentos_date):\n",
    "  def clean_pagamentos_table(info, pagamentos):\n",
    "    # Create a DataFrame that represents the primary key of base_info\n",
    "    base_info_keys = base_info_date[['id_cliente', 'safra_ref']]\n",
    "\n",
    "    # Merge with base_pagamentos, keeping only the records with matching keys\n",
    "    base_pagamentos_coherent = pd.merge(base_pagamentos_date, base_info_keys, on=['id_cliente', 'safra_ref'], how='inner')\n",
    "\n",
    "    # Count the number of excluded rows\n",
    "    excluded_rows = len(base_pagamentos_date) - len(base_pagamentos_coherent)\n",
    "    print(f'Number of excluded rows: {excluded_rows}')\n",
    "    print(f'Percentage of excluded rows: {excluded_rows / len(base_pagamentos_date) * 100:.2f}%')\n",
    "\n",
    "    # Save the coherented DataFrame to a new CSV file\n",
    "    # base_pagamentos_coherent.to_csv('base_pagamentos_desenvolvimento_coherent.csv', index=False)\n",
    "\n",
    "    print('The pagamentos table was cleaned successfully')\n",
    "    return base_pagamentos_coherent\n",
    "  base_pagamentos_date = clean_pagamentos_table(base_info_date, base_pagamentos_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a gaussian distribution of late payments for payments within -20 and 20 days of delay\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(base_pagamentos_date[(base_pagamentos_date['late_payment'] >= -20) & (base_pagamentos_date['late_payment'] <= 20)]['late_payment'], bins=np.arange(-20.5, 20.5, 1), kde=True, log_scale=(False, True))\n",
    "plt.title('Late payments')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_info_date['safra_ref'].value_counts().sort_index().plot(kind='bar', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sheets.create_sheets import create_spreadsheet, upload_csv_to_sheet\n",
    "\n",
    "spreadsheet_id = create_spreadsheet('Test Spreadsheet')\n",
    "upload_csv_to_sheet(spreadsheet_id, 'base_pagamentos_sorted_id_emissao_pagamento.csv', 'Sheet1')\n",
    "upload_csv_to_sheet(spreadsheet_id, 'base_pagamentos_drop_dupl.csv', 'Sheet2')\n",
    "# print the spreadsheet link\n",
    "print(f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/edit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grafico de linha do 50% percentile de transacoes por cliente, transacoes por cliente fraudulento e transacoes fraudulentas por cliente fraudulento\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# def concatenate_descriptions(dataframes, names):\n",
    "#     descriptions = []\n",
    "    \n",
    "#     for df, name in zip(dataframes, names):\n",
    "#         description = df.describe().to_frame().T\n",
    "#         description.index = [name]\n",
    "#         descriptions.append(description)\n",
    "    \n",
    "#     concatenated_descriptions = pd.concat(descriptions).T\n",
    "#     return concatenated_descriptions\n",
    "\n",
    "# dataframes = [transacoes_por_cliente, transacoes_por_cliente_fraudulento, fraudulent_transacoes_por_cliente_fraudulento]\n",
    "# names = ['transacoes_por_cliente', 'transacoes_por_cliente_fraudulento', 'fraudulent_transacoes_por_cliente_fraudulento']\n",
    "# transacoes_descriptions = concatenate_descriptions(dataframes, names)\n",
    "\n",
    "# # plot the line graph\n",
    "# sns.lineplot(data=transacoes_descriptions.loc[['25%', '50%', '75%'], :], dashes=False)\n",
    "# plt.title('Number of transactions per client')\n",
    "# plt.xlabel('Percentile')\n",
    "# plt.ylabel('Number of transactions')\n",
    "# plt.show()\n",
    "\n",
    "# transacoes_descriptions.head(10)\n",
    "# # base_pagamentos_date.groupby(\"id_cliente\")[\"fraud\"].sum()\n",
    "# # base_pagamentos_date[base_pagamentos_date[\"id_cliente\"] == 209314261782935157]['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clientes aparecem em base_info_date mas não em base_pagamentos_date\n",
    "clientes_info = base_info_date['id_cliente'].unique()\n",
    "clientes_pagamentos = base_pagamentos_date['id_cliente'].unique()\n",
    "clientes_info_not_in_pagamentos = [cliente for cliente in clientes_info if cliente not in clientes_pagamentos]\n",
    "clientes_pagamentos_not_in_info = [cliente for cliente in clientes_pagamentos if cliente not in clientes_info]\n",
    "print(f'Number of clientes in base_info but not in base_pagamentos: {len(clientes_info_not_in_pagamentos)}')\n",
    "print(f'Percentage of clientes in base_info but not in base_pagamentos: {len(clientes_info_not_in_pagamentos) / len(clientes_info) * 100:.2f}%\\n')\n",
    "print(f'Number of clientes in base_pagamentos but not in base_info: {len(clientes_pagamentos_not_in_info)}')\n",
    "print(f'Percentage of clientes in base_pagamentos but not in base_info: {len(clientes_pagamentos_not_in_info) / len(clientes_pagamentos) * 100:.2f}%\\n')\n",
    "\n",
    "# porcentage of transactions marked as fraudulent\n",
    "print(f'Percentage of transactions marked as fraudulent: {base_pagamentos_date[\"fraud\"].sum() / len(base_pagamentos_date) * 100:.2f}%')\n",
    "\n",
    "# porcentage of clients marked as fraudulent\n",
    "print(f'Percentage of clients marked as fraudulent at least once: {base_pagamentos_date.groupby(\"id_cliente\")[\"fraud\"].any().sum() / len(base_pagamentos_date[\"id_cliente\"].unique()) * 100:.2f}%\\n')\n",
    "\n",
    "# quantidade de clientes\n",
    "print(f'Number of clients: {len(base_pagamentos_date[\"id_cliente\"].unique())}\\n')\n",
    "\n",
    "# quantas transações cada cliente tem em base_pagamentos_date\n",
    "transacoes_por_cliente = base_pagamentos_date.groupby('id_cliente').size().sort_values(ascending=False)\n",
    "print(f'Statistics of the number of transactions per client:\\n{transacoes_por_cliente.describe()}\\n')\n",
    "\n",
    "# quantas transações cada cliente fraudulento tem em base_pagamentos_date\n",
    "transacoes_por_cliente_fraudulento = base_pagamentos_date.groupby('id_cliente').agg({'fraud': 'any', 'id_cliente': 'count'})\n",
    "transacoes_por_cliente_fraudulento.columns = ['fraud', 'transacoes']\n",
    "transacoes_por_cliente_fraudulento = transacoes_por_cliente_fraudulento[transacoes_por_cliente_fraudulento['fraud'] == 1]['transacoes'].sort_values(ascending=False)\n",
    "print(f'Statistics of the number of transactions per fraudulent client:\\n{transacoes_por_cliente_fraudulento.describe()}\\n')\n",
    "\n",
    "# quantas transações fraudulentas cada cliente fraudulento tem em base_pagamentos_date\n",
    "fraudulent_transacoes_por_cliente_fraudulento = base_pagamentos_date[base_pagamentos_date['fraud'] == 1].groupby('id_cliente').size().sort_values(ascending=False)\n",
    "print(f'Statistics of the number of fraudulent transactions per fraudulent client:\\n{fraudulent_transacoes_por_cliente_fraudulento.describe()}\\n')\n",
    "\n",
    "# porcentage of the first transaction of each client that is fraudulent\n",
    "print(f\"Percentage of the first transaction that is fraudulent: {base_pagamentos_date.sort_values(['id_cliente', 'data_emissao_documento']).drop_duplicates('id_cliente').fraud.mean() * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of late payments each client has at the time of a new borrowing\n",
    "base_pagamentos_date_test = base_pagamentos_date.copy().sort_values(['id_cliente', 'data_emissao_documento'])\n",
    "base_pagamentos_date_test['loan_count'] = base_pagamentos_date_test.groupby('id_cliente').cumcount() + 1\n",
    "df_non_expired = base_pagamentos_date_test[base_pagamentos_date_test['data_pagamento'] <= base_pagamentos_date_test['data_vencimento']]\n",
    "df_non_expired['loan_count_non_expired'] = df_non_expired.groupby('id_cliente').cumcount() + 1\n",
    "base_pagamentos_date_test = pd.merge(base_pagamentos_date_test, df_non_expired[['id_cliente', 'data_emissao_documento', 'loan_count_non_expired']], \n",
    "                                      on=['id_cliente', 'data_emissao_documento'], how='left')\n",
    "# base_pagamentos_date_test['loan_count_non_expired'] = base_pagamentos_date_test['loan_count_non_expired'].fillna(0)\n",
    "base_pagamentos_date_test.tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will calculate the compounded monthly growth rate\n",
    "def calculate_growth_rate(group):\n",
    "    group = group.sort_values(by='safra_ref')\n",
    "\n",
    "    for row in range(1, len(group)):\n",
    "        initial_value = group['renda_mes_anterior'].iloc[row]\n",
    "        final_value = group['renda_mes_anterior'].iloc[row-1]\n",
    "        start_date = group['safra_ref'].iloc[row]\n",
    "        end_date = group['safra_ref'].iloc[row-1]\n",
    "\n",
    "        # Calculate the number of periods (in months)\n",
    "        months_passed = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)\n",
    "        \n",
    "        if months_passed == 0 or initial_value == 0:\n",
    "            group['monthly_growth'] = np.nan\n",
    "        else:\n",
    "            group['monthly_growth'] = (final_value / initial_value) ** (1/months_passed) - 1\n",
    "    return group\n",
    "\n",
    "# Group by client_id\n",
    "grouped = base_info_date.groupby('id_cliente')\n",
    "base_info_date = grouped.apply(calculate_growth_rate)\n",
    "base_info_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obvio que não tem repetido pq é uma primary key\n",
    "# check if there are repeated dates for each id_cliente in base_info_date\n",
    "def check_repeated_dates(df):\n",
    "    unique_counts = df.groupby('id_cliente')['safra_ref'].nunique()\n",
    "    group_sizes = df.groupby('id_cliente').size()\n",
    "    result = unique_counts / group_sizes\n",
    "    return (result < 1).any()\n",
    "\n",
    "if check_repeated_dates(base_info_date):\n",
    "    print('There are repeated dates for some id_cliente in base_info_date')\n",
    "else:\n",
    "    print('There are no repeated dates for any id_cliente in base_info_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if every emission date is within the reference date\n",
    "base_join.apply(lambda row: (row['data_emissao_documento'].month == row['safra_ref'].month) and \n",
    "                            (row['data_emissao_documento'].year == row['safra_ref'].year), axis=1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.metrics import AUC, Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "def create_layers(input_layer, units, activation='relu', dropout_rate=0.1):\n",
    "    x_dense = Dense(units, activation=activation)(input_layer)\n",
    "    x_bn = BatchNormalization()(x_dense)\n",
    "    x_drop = Dropout(dropout_rate)(x_bn)\n",
    "    return [x_dense, x_bn, x_drop]\n",
    "\n",
    "def fraud_detection_model(input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, class_weight=None, loss_function='binary_crossentropy'):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    # encoder\n",
    "    X = create_layers(input_layer, units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X = create_layers(X[2], units)\n",
    "    X = Dense(1, activation='sigmoid')(X[2])\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=X)\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=adam, loss=loss_function, metrics=[AUC(), Precision(), Recall()])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# base_cadastral_train_eval = X_train.iloc[0:int(len(X_train)*0.85), :]\n",
    "# base_cadastral_test = X_train.iloc[int(len(X_train)*0.85):, :]\n",
    "\n",
    "# # use bayes optimization to find the best combination of lr, wd, class_weight, and loss function\n",
    "# model = fraud_detection_model(input_dim=input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, loss_function='binary_crossentropy')\n",
    "# history = model.fit(X_train, y_train, epochs=30, batch_size=512, validation_split=0.2, class_weight={0:1, 1:1})\n",
    "\n",
    "# # autoencoder.evaluate(base_cadastral_test, base_cadastral_test)\n",
    "\n",
    "# y_pred = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "histories = []\n",
    "params_list = []\n",
    "\n",
    "# define the search space for hyperparameters\n",
    "space = {\n",
    "    'units': hp.choice('units', [15, 30, 60]), # sample units per layer\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4*np.log(10), -2*np.log(10)),  # vary learning rate on a log scale\n",
    "    'weight_decay': hp.loguniform('weight_decay', -5*np.log(10), -3*np.log(10)),    # vary weight decay on a log scale\n",
    "    'class_weight': hp.choice('class_weight', [{0:1, 1:1}, {0:1, 1:2}]),  # sample class weights\n",
    "    'loss_function': hp.choice('loss_function', ['binary_crossentropy', BinaryFocalCrossentropy(gamma=1)])  # sample loss functions\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    # create the model\n",
    "    model = fraud_detection_model(input_dim=input_dim, **params)\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=512, validation_split=0.2, class_weight=params['class_weight'])\n",
    "\n",
    "    # save the history and parameters\n",
    "    histories.append(history)\n",
    "    params_list.append(params)\n",
    "\n",
    "    # calculate the loss as the average of the last 3 epochs\n",
    "    validation_loss = np.mean(history.history['val_loss'][-5:])\n",
    "\n",
    "    # return the loss\n",
    "    return {'loss': validation_loss, 'status': STATUS_OK}\n",
    "\n",
    "# create a trials object\n",
    "trials = Trials()\n",
    "\n",
    "# find the best hyperparameters\n",
    "best = fmin(\n",
    "    fn=objective,  # objective function\n",
    "    space=space,  # hyperparameter space\n",
    "    algo=tpe.suggest,  # surrogate algorithm\n",
    "    max_evals=8,  # number of iterations\n",
    "    trials=trials,  # trials object to store details of the iteration\n",
    "    rstate=np.random.default_rng(1)  # for reproducibility\n",
    ")\n",
    "\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories.append(history)\n",
    "\n",
    "# loss: 0.1509 - auc_8: 0.8766 - precision_8: 0.7465 - recall_8: 0.3551 - binary_accuracy: 0.9517 - val_loss: 0.6307 - val_auc_8: 0.8885 - val_precision_8: 0.0889 - val_recall_8: 0.8822 - val_binary_accuracy: 0.5962 before\n",
    "# loss: 0.0440 - auc_9: 0.9928 - precision_9: 0.8875 - recall_9: 0.8423 - binary_accuracy: 0.9833 - val_loss: 0.0422 - val_auc_9: 0.9972 - val_precision_9: 0.9939 - val_recall_9: 0.6293 - val_binary_accuracy: 0.9835 as in 3 but extra layer\n",
    "# loss: 0.0515 - auc_10: 0.8003 - precision_10: 0.7173 - recall_10: 0.1958 - binary_accuracy: 0.9444 - val_loss: 1.5347 - val_auc_10: 0.7662 - val_precision_10: 0.0447 - val_recall_10: 0.9942 - val_binary_accuracy: 0.0627 gamma=2\n",
    "# loss: 0.0458 - auc_11: 0.9708 - precision_11: 0.8109 - recall_11: 0.6371 - binary_accuracy: 0.9677 - val_loss: 0.1299 - val_auc_11: 0.7191 - val_precision_11: 1.0000 - val_recall_11: 0.1699 - val_binary_accuracy: 0.9634 gamma=1\n",
    "# loss: 0.5288 - auc_14: 0.9595 - precision_14: 0.2976 - recall_14: 0.9596 - binary_accuracy: 0.8545 - val_loss: 0.0925 - val_auc_14: 0.9189 - val_precision_14: 0.8729 - val_recall_14: 0.5039 - val_binary_accuracy: 0.9749 class_weight={0:1, 1:20}\n",
    "# loss: 0.0699 - auc_15: 0.9925 - precision_15: 0.8263 - recall_15: 0.8908 - binary_accuracy: 0.9813 - val_loss: 0.3131 - val_auc_15: 0.9623 - val_precision_15: 0.4399 - val_recall_15: 0.8687 - val_binary_accuracy: 0.9455 class_weight={0:1, 1:2}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = fig.add_subplot(4, 4, i+1)\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    # ax.plot(history.history['recall'])\n",
    "    # ax.plot(history.history['val_recall'])\n",
    "    title = f'u{ params_list[i][\"units\"]}, lr {params_list[i][\"learning_rate\"]:.2e}, wd {params_list[i][\"weight_decay\"]:.2e}, cw {params_list[i][\"class_weight\"][1]}'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation', 'recall', 'val_recall'])\n",
    "plt.show()\n",
    "\n",
    "# 1:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 256, epochs 100, Total params: 1,441\n",
    "# 2:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,441\n",
    "# 3:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,441\n",
    "# 4:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.003, wd 0.0001, batch 512, epochs 30, Total params: 1,441\n",
    "# 5:   cl 30, cl 30, cl 30, cl 30, cl 30 dense 1 sigmoid, lr 0.003, wd 0.0001, batch 512, epochs 30, Total params: 4,681\n",
    "# 6:   cl 30, cl 30, cl 30, cl 30, cl 30 dense 1 sigmoid, lr 0.003, wd 0.001, batch 512, epochs 30, Total params: 4,681\n",
    "# 7:   cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741\n",
    "# 8:   cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741\n",
    "# 9:   cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=2\n",
    "# 10:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=1\n",
    "# 11:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741\n",
    "# 12:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=1class_weight={0:1, 1:20}\n",
    "# 13:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=1class_weight={0:1, 1:2}\n",
    "\n",
    "# # save the collection of histories to a file\n",
    "# import pickle\n",
    "# with open('histories.pickle', 'wb') as f:\n",
    "#     pickle.dump(histories, f)\n",
    "\n",
    "# # load the collection of histories from a file\n",
    "# import pickle\n",
    "# with open('histories.pickle', 'rb') as f:\n",
    "#     histories_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# loss: 0.3269 - mae: 0.3568 - val_loss: 0.4145 - val_mae: 0.3445\n",
    "# loss: 0.5927 - mae: 0.4031\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.metrics import AUC, Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "def create_layers(input_layer, units, activation='relu', dropout_rate=0.1):\n",
    "    x_dense = Dense(units, activation=activation)(input_layer)\n",
    "    x_bn = BatchNormalization()(x_dense)\n",
    "    x_drop = Dropout(dropout_rate)(x_bn)\n",
    "    return [x_dense, x_bn, x_drop]\n",
    "\n",
    "def fraud_detection_model(input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, class_weight=None, loss_function='binary_crossentropy'):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    # encoder\n",
    "    X = create_layers(input_layer, units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X = create_layers(X[2], units)\n",
    "    X = Dense(1, activation='sigmoid')(X[2])\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=X)\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=adam, loss=loss_function, metrics=[AUC(), Precision(), Recall()])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# base_cadastral_train_eval = X_train.iloc[0:int(len(X_train)*0.85), :]\n",
    "# base_cadastral_test = X_train.iloc[int(len(X_train)*0.85):, :]\n",
    "\n",
    "# # use bayes optimization to find the best combination of lr, wd, class_weight, and loss function\n",
    "# model = fraud_detection_model(input_dim=input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, loss_function='binary_crossentropy')\n",
    "# history = model.fit(X_train, y_train, epochs=30, batch_size=512, validation_split=0.2, class_weight={0:1, 1:1})\n",
    "\n",
    "# # autoencoder.evaluate(base_cadastral_test, base_cadastral_test)\n",
    "\n",
    "# y_pred = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "histories = []\n",
    "params_list = []\n",
    "\n",
    "# define the search space for hyperparameters\n",
    "space = {\n",
    "    'units': hp.choice('units', [15, 30, 60]), # sample units per layer\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4*np.log(10), -2*np.log(10)),  # vary learning rate on a log scale\n",
    "    'weight_decay': hp.loguniform('weight_decay', -5*np.log(10), -3*np.log(10)),    # vary weight decay on a log scale\n",
    "    'class_weight': hp.choice('class_weight', [{0:1, 1:1}, {0:1, 1:2}]),  # sample class weights\n",
    "    'loss_function': hp.choice('loss_function', ['binary_crossentropy', BinaryFocalCrossentropy(gamma=1)])  # sample loss functions\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    # create the model\n",
    "    model = fraud_detection_model(input_dim=input_dim, **params)\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=30, batch_size=512, validation_split=0.2, class_weight=params['class_weight'])\n",
    "\n",
    "    # save the history and parameters\n",
    "    histories.append(history)\n",
    "    params_list.append(params)\n",
    "\n",
    "    # calculate the loss as the average of the last 3 epochs\n",
    "    validation_loss = np.mean(history.history['val_loss'][-5:])\n",
    "\n",
    "    # return the loss\n",
    "    return {'loss': validation_loss, 'status': STATUS_OK}\n",
    "\n",
    "# create a trials object\n",
    "trials = Trials()\n",
    "\n",
    "# find the best hyperparameters\n",
    "best = fmin(\n",
    "    fn=objective,  # objective function\n",
    "    space=space,  # hyperparameter space\n",
    "    algo=tpe.suggest,  # surrogate algorithm\n",
    "    max_evals=8,  # number of iterations\n",
    "    trials=trials,  # trials object to store details of the iteration\n",
    "    rstate=np.random.default_rng(1)  # for reproducibility\n",
    ")\n",
    "\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histories.append(history)\n",
    "\n",
    "# loss: 0.1509 - auc_8: 0.8766 - precision_8: 0.7465 - recall_8: 0.3551 - binary_accuracy: 0.9517 - val_loss: 0.6307 - val_auc_8: 0.8885 - val_precision_8: 0.0889 - val_recall_8: 0.8822 - val_binary_accuracy: 0.5962 before\n",
    "# loss: 0.0440 - auc_9: 0.9928 - precision_9: 0.8875 - recall_9: 0.8423 - binary_accuracy: 0.9833 - val_loss: 0.0422 - val_auc_9: 0.9972 - val_precision_9: 0.9939 - val_recall_9: 0.6293 - val_binary_accuracy: 0.9835 as in 3 but extra layer\n",
    "# loss: 0.0515 - auc_10: 0.8003 - precision_10: 0.7173 - recall_10: 0.1958 - binary_accuracy: 0.9444 - val_loss: 1.5347 - val_auc_10: 0.7662 - val_precision_10: 0.0447 - val_recall_10: 0.9942 - val_binary_accuracy: 0.0627 gamma=2\n",
    "# loss: 0.0458 - auc_11: 0.9708 - precision_11: 0.8109 - recall_11: 0.6371 - binary_accuracy: 0.9677 - val_loss: 0.1299 - val_auc_11: 0.7191 - val_precision_11: 1.0000 - val_recall_11: 0.1699 - val_binary_accuracy: 0.9634 gamma=1\n",
    "# loss: 0.5288 - auc_14: 0.9595 - precision_14: 0.2976 - recall_14: 0.9596 - binary_accuracy: 0.8545 - val_loss: 0.0925 - val_auc_14: 0.9189 - val_precision_14: 0.8729 - val_recall_14: 0.5039 - val_binary_accuracy: 0.9749 class_weight={0:1, 1:20}\n",
    "# loss: 0.0699 - auc_15: 0.9925 - precision_15: 0.8263 - recall_15: 0.8908 - binary_accuracy: 0.9813 - val_loss: 0.3131 - val_auc_15: 0.9623 - val_precision_15: 0.4399 - val_recall_15: 0.8687 - val_binary_accuracy: 0.9455 class_weight={0:1, 1:2}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = fig.add_subplot(4, 4, i+1)\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    # ax.plot(history.history['recall'])\n",
    "    # ax.plot(history.history['val_recall'])\n",
    "    title = f'u{ params_list[i][\"units\"]}, lr {params_list[i][\"learning_rate\"]:.2e}, wd {params_list[i][\"weight_decay\"]:.2e}, cw {params_list[i][\"class_weight\"][1]}'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation', 'recall', 'val_recall'])\n",
    "plt.show()\n",
    "\n",
    "# 1:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 256, epochs 100, Total params: 1,441\n",
    "# 2:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,441\n",
    "# 3:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,441\n",
    "# 4:   cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.003, wd 0.0001, batch 512, epochs 30, Total params: 1,441\n",
    "# 5:   cl 30, cl 30, cl 30, cl 30, cl 30 dense 1 sigmoid, lr 0.003, wd 0.0001, batch 512, epochs 30, Total params: 4,681\n",
    "# 6:   cl 30, cl 30, cl 30, cl 30, cl 30 dense 1 sigmoid, lr 0.003, wd 0.001, batch 512, epochs 30, Total params: 4,681\n",
    "# 7:   cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741\n",
    "# 8:   cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741\n",
    "# 9:   cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=2\n",
    "# 10:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=1\n",
    "# 11:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741\n",
    "# 12:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=1class_weight={0:1, 1:20}\n",
    "# 13:  cl 15, cl 15, cl 15, cl 15, cl 15, cl 15 dense 1 sigmoid, lr 0.001, wd 0.0001, batch 512, epochs 30, Total params: 1,741, gama=1class_weight={0:1, 1:2}\n",
    "\n",
    "# # save the collection of histories to a file\n",
    "# import pickle\n",
    "# with open('histories.pickle', 'wb') as f:\n",
    "#     pickle.dump(histories, f)\n",
    "\n",
    "# # load the collection of histories from a file\n",
    "# import pickle\n",
    "# with open('histories.pickle', 'rb') as f:\n",
    "#     histories_loaded = pickle.load(f)\n",
    "\n",
    "\n",
    "# loss: 0.3269 - mae: 0.3568 - val_loss: 0.4145 - val_mae: 0.3445\n",
    "# loss: 0.5927 - mae: 0.4031\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.metrics import AUC, Precision, Recall, BinaryAccuracy\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "def create_layers(input_layer, units, activation='relu', dropout_rate=0.1):\n",
    "    x_dense = Dense(units, activation=activation)(input_layer)\n",
    "    x_bn = BatchNormalization()(x_dense)\n",
    "    x_drop = Dropout(dropout_rate)(x_bn)\n",
    "    return [x_dense, x_bn, x_drop]\n",
    "\n",
    "def fraud_detection_model(input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, loss_function='binary_crossentropy'):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    # encoder\n",
    "    X = create_layers(input_layer, units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X = create_layers(X[2], units)\n",
    "    X = Dense(1, activation='sigmoid')(X[2])\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=X)\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=adam, loss=loss_function, metrics=[AUC(), Precision(), Recall()])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# # use bayes optimization to find the best combination of lr, wd, class_weight, and loss function\n",
    "\n",
    "# model = fraud_detection_model(input_dim=input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, loss_function='binary_crossentropy')\n",
    "# history = model.fit(X_train, y_train, epochs=30, batch_size=512, validation_split=0.2, class_weight={0:1, 1:1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# define a function to create the model\n",
    "def create_model(units=15, learning_rate=0.001, weight_decay=0.0, class_weight={0:1, 1:1}, loss_function='binary_crossentropy'):\n",
    "    model = fraud_detection_model(input_dim=input_dim, \n",
    "                                  units=units,\n",
    "                                  learning_rate=learning_rate, \n",
    "                                  weight_decay=weight_decay, \n",
    "                                  loss_function=loss_function)\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=30, batch_size=512)\n",
    "\n",
    "# define the search spaces\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(1e-5, 1e-1, prior='log-uniform'),\n",
    "    'weight_decay': Real(1e-5, 1e-1, prior='log-uniform'),\n",
    "    'class_weight': Categorical([{0:1, 1:1}, {0:1, 1:2}, {0:1, 1:3}, {0:1, 1:4}, {0:1, 1:5}]),\n",
    "    'loss_function': Categorical(['binary_crossentropy', 'hinge'])\n",
    "}\n",
    "\n",
    "# define the cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# create the BayesSearchCV object\n",
    "opt = BayesSearchCV(model,\n",
    "                    search_spaces,\n",
    "                    n_iter=50,\n",
    "                    cv=cv,\n",
    "                    n_jobs=1,  # use parallel computing if possible\n",
    "                    random_state=42)\n",
    "\n",
    "# perform the search\n",
    "opt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class IsolationForestWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=10, max_samples='auto', contamination=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.contamination = contamination\n",
    "        self.model = IsolationForest(n_estimators=self.n_estimators, \n",
    "                                     max_samples=self.max_samples, \n",
    "                                     contamination=self.contamination)\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.y_train = (y == 1).astype(int) if y is not None else None\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = self.model.predict(X)\n",
    "        return (pred == 1).astype(int)\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        if y is not None:\n",
    "            y = (y == 1).astype(int)\n",
    "            y_pred = self.predict(X)\n",
    "            return f1_score(y, y_pred)\n",
    "        elif self.y_train is not None:\n",
    "            y_pred = self.predict(X)\n",
    "            return f1_score(self.y_train, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"No y values were given during fitting, cannot calculate score.\")\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        super().set_params(**params)\n",
    "        self.model.set_params(**params)\n",
    "        return self\n",
    "\n",
    "iso_forest = IsolationForestWrapper(n_estimators=30)\n",
    "\n",
    "params = {\n",
    "    'contamination': np.linspace(0.01, 0.2, 20)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(iso_forest, params, n_iter=25, cv=3, n_jobs=-1)\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = opt.predict(X_eval)\n",
    "\n",
    "print(confusion_matrix(y_eval, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "precision = precision_score(y_eval, y_pred)\n",
    "recall = recall_score(y_eval, y_pred)\n",
    "f1 = f1_score(y_eval, y_pred)\n",
    "roc_auc = roc_auc_score(y_eval, y_pred)\n",
    "\n",
    "with open('results.txt', 'a') as f:\n",
    "    print(f\"Best parameters for IsolationForest: {opt.best_params_}\", file=f)\n",
    "    print(f\"Classifier: IsolationForest\", file=f)\n",
    "    print(f\"Accuracy: {accuracy}\", file=f)\n",
    "    print(f\"Precision: {precision}\", file=f)\n",
    "    print(f\"Recall: {recall}\", file=f)\n",
    "    print(f\"F1 Score: {f1}\", file=f)\n",
    "    print(f\"ROC AUC: {roc_auc}\", file=f)\n",
    "    print(\"------------------------\", file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop to print the loss and the metrics of all the models history in the histories list\n",
    "# for i in range(len(histories)):\n",
    "#     with open('nn_results.txt', 'a') as f:\n",
    "#         print(f\"Parameters: {params_list[i]}\", file=f)\n",
    "#         history_keys = list(histories[i].history.keys())\n",
    "#         print(f\"Loss: {np.round(np.mean(histories[i].history[history_keys[0]][-3:]),4)} --> {np.round(np.mean(histories[i].history[history_keys[4]][-3:]),4)}\", file=f)\n",
    "#         print(f\"AUC: {np.round(np.mean(histories[i].history[history_keys[1]][-3:]),4)} --> {np.round(np.mean(histories[i].history[history_keys[5]][-3:]),4)}\", file=f)\n",
    "#         print(f\"Precision: {np.round(np.mean(histories[i].history[history_keys[2]][-3:]),4)} --> {np.round(np.mean(histories[i].history[history_keys[6]][-3:]),4)}\", file=f)\n",
    "#         print(f\"Recall: {np.round(np.mean(histories[i].history[history_keys[3]][-3:]),4)} --> {np.round(np.mean(histories[i].history[history_keys[7]][-3:]),4)}\", file=f)\n",
    "#         print(\"------------------------\", file=f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
