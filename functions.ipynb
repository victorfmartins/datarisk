{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PCA\n",
    "from sklearn.decomposition import PCA\n",
    "base_cadastral_pca = PCA(n_components=3).fit_transform(base_cadastral_scaled)\n",
    "print(base_cadastral_pca.shape)\n",
    "\n",
    "# use UMAP to reduce dimensionality\n",
    "import umap\n",
    "base_cadastral_umap = umap.UMAP(n_components=3).fit_transform(base_cadastral_scaled)\n",
    "print(base_cadastral_umap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the PCA\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(base_cadastral_pca[:, 0], base_cadastral_pca[:, 1], base_cadastral_pca[:, 2], c=c_column, cmap='coolwarm')\n",
    "plt.title('PCA')\n",
    "plt.show()\n",
    "\n",
    "# plot the UMAP\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(base_cadastral_umap[:, 0], base_cadastral_umap[:, 1], base_cadastral_umap[:, 2], c=c_column, cmap='coolwarm')\n",
    "plt.title('UMAP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show that ddd and cep_2_dig are not good features using chi2 test\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "X = base_cadastral_scaled.drop(['fraud'], axis=1)\n",
    "y = base_cadastral_scaled['fraud']  \n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)\n",
    "print(X_kbest.shape)\n",
    "print(chi2_selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fraud_dependent_of_the_data_exclusive_to_pagamentos_table(info, pagamentos):\n",
    "  info = info.copy()\n",
    "  pagamentos = pagamentos.copy()\n",
    "\n",
    "  # lable as 1 all rows of pagamentos where the pair (id_cliente, safra_ref) is in info and as 0 otherwise\n",
    "  pagamentos['id_cliente_safra_ref'] = pagamentos['id_cliente'].astype(str) + '_' + pagamentos['safra_ref'].astype(str)\n",
    "  info['id_cliente_safra_ref'] = info['id_cliente'].astype(str) + '_' + info['safra_ref'].astype(str)\n",
    "  pagamentos['coherent'] = np.where(pagamentos['id_cliente_safra_ref'].isin(info['id_cliente_safra_ref']), 1, 0)\n",
    "\n",
    "  # Chi-square test to determine if the fraud is independent of the coherent column\n",
    "  from scipy.stats import chi2_contingency\n",
    "  contingency_table = pd.crosstab(pagamentos['fraud'], pagamentos['coherent'])\n",
    "  stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "  if p < 0.05:\n",
    "    print('The fraud is dependent of the data in pagamentos where its key does not appear in the info table.\\n' +\n",
    "          'This data should be kept in the pagamentos table')\n",
    "  else:\n",
    "    print('The fraud is independent of the data in pagamentos where its key does not appear in the info table\\n' +\n",
    "          'This data can be removed from the pagamentos table')\n",
    "  return p < 0.05\n",
    "\n",
    "if not is_fraud_dependent_of_the_data_exclusive_to_pagamentos_table(base_info_date, base_pagamentos_date):\n",
    "  def clean_pagamentos_table(info, pagamentos):\n",
    "    # Create a DataFrame that represents the primary key of base_info\n",
    "    base_info_keys = base_info_date[['id_cliente', 'safra_ref']]\n",
    "\n",
    "    # Merge with base_pagamentos, keeping only the records with matching keys\n",
    "    base_pagamentos_coherent = pd.merge(base_pagamentos_date, base_info_keys, on=['id_cliente', 'safra_ref'], how='inner')\n",
    "\n",
    "    # Count the number of excluded rows\n",
    "    excluded_rows = len(base_pagamentos_date) - len(base_pagamentos_coherent)\n",
    "    print(f'Number of excluded rows: {excluded_rows}')\n",
    "    print(f'Percentage of excluded rows: {excluded_rows / len(base_pagamentos_date) * 100:.2f}%')\n",
    "\n",
    "    # Save the coherented DataFrame to a new CSV file\n",
    "    # base_pagamentos_coherent.to_csv('base_pagamentos_desenvolvimento_coherent.csv', index=False)\n",
    "\n",
    "    print('The pagamentos table was cleaned successfully')\n",
    "    return base_pagamentos_coherent\n",
    "  base_pagamentos_date = clean_pagamentos_table(base_info_date, base_pagamentos_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a gaussian distribution of late payments for payments within -20 and 20 days of delay\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(base_pagamentos_date[(base_pagamentos_date['late_payment'] >= -20) & (base_pagamentos_date['late_payment'] <= 20)]['late_payment'], bins=np.arange(-20.5, 20.5, 1), kde=True, log_scale=(False, True))\n",
    "plt.title('Late payments')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_info_date['safra_ref'].value_counts().sort_index().plot(kind='bar', figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from create_sheets import create_spreadsheet, upload_csv_to_sheet\n",
    "\n",
    "# spreadsheet_id = create_spreadsheet('Test Spreadsheet')\n",
    "# upload_csv_to_sheet(spreadsheet_id, 'base_pagamentos_sorted_id_emissao_pagamento.csv', 'Sheet1')\n",
    "# upload_csv_to_sheet(spreadsheet_id, 'base_pagamentos_drop_dupl.csv', 'Sheet2')\n",
    "# # print the spreadsheet link\n",
    "# print(f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/edit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # grafico de linha do 50% percentile de transacoes por cliente, transacoes por cliente fraudulento e transacoes fraudulentas por cliente fraudulento\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# def concatenate_descriptions(dataframes, names):\n",
    "#     descriptions = []\n",
    "    \n",
    "#     for df, name in zip(dataframes, names):\n",
    "#         description = df.describe().to_frame().T\n",
    "#         description.index = [name]\n",
    "#         descriptions.append(description)\n",
    "    \n",
    "#     concatenated_descriptions = pd.concat(descriptions).T\n",
    "#     return concatenated_descriptions\n",
    "\n",
    "# dataframes = [transacoes_por_cliente, transacoes_por_cliente_fraudulento, fraudulent_transacoes_por_cliente_fraudulento]\n",
    "# names = ['transacoes_por_cliente', 'transacoes_por_cliente_fraudulento', 'fraudulent_transacoes_por_cliente_fraudulento']\n",
    "# transacoes_descriptions = concatenate_descriptions(dataframes, names)\n",
    "\n",
    "# # plot the line graph\n",
    "# sns.lineplot(data=transacoes_descriptions.loc[['25%', '50%', '75%'], :], dashes=False)\n",
    "# plt.title('Number of transactions per client')\n",
    "# plt.xlabel('Percentile')\n",
    "# plt.ylabel('Number of transactions')\n",
    "# plt.show()\n",
    "\n",
    "# transacoes_descriptions.head(10)\n",
    "# # base_pagamentos_date.groupby(\"id_cliente\")[\"fraud\"].sum()\n",
    "# # base_pagamentos_date[base_pagamentos_date[\"id_cliente\"] == 209314261782935157]['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clientes aparecem em base_info_date mas não em base_pagamentos_date\n",
    "clientes_info = base_info_date['id_cliente'].unique()\n",
    "clientes_pagamentos = base_pagamentos_date['id_cliente'].unique()\n",
    "clientes_info_not_in_pagamentos = [cliente for cliente in clientes_info if cliente not in clientes_pagamentos]\n",
    "clientes_pagamentos_not_in_info = [cliente for cliente in clientes_pagamentos if cliente not in clientes_info]\n",
    "print(f'Number of clientes in base_info but not in base_pagamentos: {len(clientes_info_not_in_pagamentos)}')\n",
    "print(f'Percentage of clientes in base_info but not in base_pagamentos: {len(clientes_info_not_in_pagamentos) / len(clientes_info) * 100:.2f}%\\n')\n",
    "print(f'Number of clientes in base_pagamentos but not in base_info: {len(clientes_pagamentos_not_in_info)}')\n",
    "print(f'Percentage of clientes in base_pagamentos but not in base_info: {len(clientes_pagamentos_not_in_info) / len(clientes_pagamentos) * 100:.2f}%\\n')\n",
    "\n",
    "# porcentage of transactions marked as fraudulent\n",
    "print(f'Percentage of transactions marked as fraudulent: {base_pagamentos_date[\"fraud\"].sum() / len(base_pagamentos_date) * 100:.2f}%')\n",
    "\n",
    "# porcentage of clients marked as fraudulent\n",
    "print(f'Percentage of clients marked as fraudulent at least once: {base_pagamentos_date.groupby(\"id_cliente\")[\"fraud\"].any().sum() / len(base_pagamentos_date[\"id_cliente\"].unique()) * 100:.2f}%\\n')\n",
    "\n",
    "# quantidade de clientes\n",
    "print(f'Number of clients: {len(base_pagamentos_date[\"id_cliente\"].unique())}\\n')\n",
    "\n",
    "# quantas transações cada cliente tem em base_pagamentos_date\n",
    "transacoes_por_cliente = base_pagamentos_date.groupby('id_cliente').size().sort_values(ascending=False)\n",
    "print(f'Statistics of the number of transactions per client:\\n{transacoes_por_cliente.describe()}\\n')\n",
    "\n",
    "# quantas transações cada cliente fraudulento tem em base_pagamentos_date\n",
    "transacoes_por_cliente_fraudulento = base_pagamentos_date.groupby('id_cliente').agg({'fraud': 'any', 'id_cliente': 'count'})\n",
    "transacoes_por_cliente_fraudulento.columns = ['fraud', 'transacoes']\n",
    "transacoes_por_cliente_fraudulento = transacoes_por_cliente_fraudulento[transacoes_por_cliente_fraudulento['fraud'] == 1]['transacoes'].sort_values(ascending=False)\n",
    "print(f'Statistics of the number of transactions per fraudulent client:\\n{transacoes_por_cliente_fraudulento.describe()}\\n')\n",
    "\n",
    "# quantas transações fraudulentas cada cliente fraudulento tem em base_pagamentos_date\n",
    "fraudulent_transacoes_por_cliente_fraudulento = base_pagamentos_date[base_pagamentos_date['fraud'] == 1].groupby('id_cliente').size().sort_values(ascending=False)\n",
    "print(f'Statistics of the number of fraudulent transactions per fraudulent client:\\n{fraudulent_transacoes_por_cliente_fraudulento.describe()}\\n')\n",
    "\n",
    "# porcentage of the first transaction of each client that is fraudulent\n",
    "print(f\"Percentage of the first transaction that is fraudulent: {base_pagamentos_date.sort_values(['id_cliente', 'data_emissao_documento']).drop_duplicates('id_cliente').fraud.mean() * 100:.2f}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
