{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Datarisk - Detecção de Fraudes\n",
    "\n",
    "Foram fornecidas os arquivos base_info.csv, base_cadastral.csv e base_pagamentos.csv.\n",
    "- base_cadastral possui informações fixas (cadastrais) sobre cada cliente.\n",
    "- base_info possui informações mensais de renda e número de empregados.\n",
    "- base_pagamentos possui informações referentes a cada operação de crédito."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento e Visualização dos Dados\n",
    "- leitura dos arquivos e ajuste de cada tipo de dado.\n",
    "- codificação (one_hot, categorica e ordinal) dos dados cadastrais.\n",
    "- visualização dos dados do cliente reduzido para 3 dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leitura dos arquivos e ajuste de cada tipo de dado.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot  as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data from the CSV files\n",
    "base_info = pd.read_csv('base_info.csv')\n",
    "base_pagamentos = pd.read_csv('base_pagamentos_desenvolvimento.csv')\n",
    "base_pagamentos_teste = pd.read_csv('base_pagamentos_teste.csv')\n",
    "base_cadastral = pd.read_csv('base_cadastral.csv')\n",
    "\n",
    "# lower case all column names\n",
    "base_info.columns = base_info.columns.str.lower()\n",
    "base_pagamentos.columns = base_pagamentos.columns.str.lower()\n",
    "base_pagamentos_teste.columns = base_pagamentos_teste.columns.str.lower()\n",
    "base_cadastral.columns = base_cadastral.columns.str.lower()\n",
    "\n",
    "# give a new id to each client based on registrartion date order\n",
    "base_cadastral = base_cadastral.sort_values('data_cadastro').reset_index(drop=True)\n",
    "base_cadastral['new_id'] = range(1, len(base_cadastral) + 1)\n",
    "\n",
    "# save a corresponding dictionary of old_id and new_id\n",
    "id_dict = base_cadastral[['id_cliente', 'new_id']].set_index('id_cliente').to_dict()\n",
    "\n",
    "# convert str to date\n",
    "def convert_date(df, to_date_columns, format):\n",
    "  df_copy = df.copy()\n",
    "  for col in to_date_columns:\n",
    "    df_copy[col] = pd.to_datetime(df_copy[col], format=format)#.dt.date\n",
    "  return df_copy\n",
    "\n",
    "date_columns = ['data_emissao_documento', 'data_pagamento', 'data_vencimento']\n",
    "base_pagamentos_date = convert_date(base_pagamentos, date_columns, '%Y-%m-%d')\n",
    "base_pagamentos_date = convert_date(base_pagamentos_date, ['safra_ref'], '%Y-%m')\n",
    "date_columns = ['data_emissao_documento', 'data_vencimento']\n",
    "base_pagamentos_teste_date = convert_date(base_pagamentos_teste, date_columns, '%Y-%m-%d')\n",
    "base_pagamentos_teste_date = convert_date(base_pagamentos_teste_date, ['safra_ref'], '%Y-%m')\n",
    "base_info_date = convert_date(base_info, ['safra_ref'], '%Y-%m')\n",
    "base_cadastral_date = convert_date(base_cadastral, ['data_cadastro'], '%Y-%m-%d')\n",
    "\n",
    "# ajust values in base_cadastral\n",
    "base_cadastral_date.flag_pf = (base_cadastral_date.flag_pf == 'X').astype(int)\n",
    "base_cadastral_date.segmento_industrial = base_cadastral_date.segmento_industrial.fillna('NAN')\n",
    "base_cadastral_date.dominio_email = base_cadastral_date.dominio_email.fillna('NAN')\n",
    "base_cadastral_date.porte = base_cadastral_date.porte.fillna('NAN')\n",
    "base_cadastral_date.cep_2_dig = base_cadastral_date.cep_2_dig.fillna('NA')\n",
    "base_cadastral_date.ddd = base_cadastral_date.ddd.fillna('NAN')\n",
    "base_cadastral_date.loc[base_cadastral_date['ddd'].str.contains(\"\\(\"), 'ddd'] = 'INVALID'\n",
    "\n",
    "# Add a binary column, fraud, to base_pagamentos_desenvolvimento_coherent and assing 1 to rows where DATA_PAGAMENTO > DATA_VENCIMENTO + 5\n",
    "base_pagamentos_date['late_payment'] = (base_pagamentos_date['data_pagamento'] - base_pagamentos_date['data_vencimento']).dt.days\n",
    "base_pagamentos_date['fraud'] = np.where(base_pagamentos_date['late_payment'] > 5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## codificação (one_hot, categorica e ordinal) dos dados da base cadastral\n",
    "\n",
    "# date encoding\n",
    "def encode_date(df, columns):\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        min_date = df_copy[col].min()\n",
    "        df_copy[col + '_since_min'] = (df_copy[col] - min_date).dt.days + 1\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "def encode_categorical(df, columns):\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        df_copy = pd.concat([df_copy, pd.get_dummies(df_copy[col], prefix=col)], axis=1)\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "# ordinal encoding for categorical variables\n",
    "def encode_ordinal(df, columns, mapping):\n",
    "    df_copy = df.copy()\n",
    "    for i, col in enumerate(columns):\n",
    "        df_copy[col + '_encoded'] = df_copy[col].map(mapping[i])\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "ordinal_encode_map = [{'NAN': 0, 'PEQUENO': 1, 'MEDIO': 2, 'GRANDE': 3}]\n",
    "base_cadastral_date_encoded = encode_date(base_cadastral_date, ['data_cadastro'])\n",
    "base_cadastral_one_hot_encoded = encode_categorical(base_cadastral_date_encoded, ['segmento_industrial', 'dominio_email'])\n",
    "base_cadastral_ordinal_encoded = encode_ordinal(base_cadastral_one_hot_encoded, ['porte'], ordinal_encode_map)\n",
    "base_cadastral_droped = base_cadastral_ordinal_encoded.drop(['id_cliente', 'ddd', 'cep_2_dig'], axis=1)\n",
    "base_cadastral_droped\n",
    "\n",
    "# standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "base_cadastral_scaled = scaler.fit_transform(base_cadastral_droped)\n",
    "base_cadastral_scaled = pd.DataFrame(base_cadastral_scaled, columns=base_cadastral_droped.columns)\n",
    "# display(base_cadastral_scaled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualização dos dados do cliente reduzido para 3 dimensões\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "base_cadastral_tsne = TSNE(n_components=3).fit_transform(base_cadastral_scaled)\n",
    "print(base_cadastral_tsne.shape)\n",
    "\n",
    "# plot 16 TSNEs with different columns as color\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(base_cadastral_scaled.columns):\n",
    "    ax = fig.add_subplot(4, 4, i+1, projection='3d')\n",
    "    ax.scatter(base_cadastral_tsne[:, 0], \n",
    "               base_cadastral_tsne[:, 1], \n",
    "               base_cadastral_tsne[:, 2], \n",
    "               c=base_cadastral_scaled[col], \n",
    "               cmap='coolwarm')\n",
    "    plt.title(col)\n",
    "plt.show()\n",
    "\n",
    "base_cadastral_tsne_df = pd.DataFrame(base_cadastral_tsne, columns=['tsne_1', 'tsne_2', 'tsne_3'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificação (compressão) da informação de cada cliente na base cadastral\n",
    "- codificação via modelo autoencoder para redução de dimensionalidade de 15 para 3.\n",
    "- visualização do treinamento do modelo.\n",
    "- análise de erro do modelo.\n",
    "- visualização do cliente em 3 dimensões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## codificação via modelo autoencoder para redução de dimensionalidade de 15 para 3\n",
    "\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def create_layers(input_layer, units, activation='relu', dropout_rate=0.1):\n",
    "    x_dense = Dense(units, activation=activation)(input_layer)\n",
    "    x_bn = BatchNormalization()(x_dense)\n",
    "    x_drop = Dropout(dropout_rate)(x_bn)\n",
    "    return [x_dense, x_bn, x_drop]\n",
    "\n",
    "def autoencoder_model(input_dim, encoding_dim, learning_rate=0.002, weight_decay=0.0001):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    # encoder\n",
    "    encoder = create_layers(input_layer, 25)\n",
    "    encoder = create_layers(encoder[2], 15)\n",
    "    encoder = create_layers(encoder[2] + input_layer, 15)\n",
    "    encoder_skip = create_layers(encoder[2], 15)\n",
    "    encoder = create_layers(encoder_skip[2] + encoder[1], 15)\n",
    "    encoded = Dense(encoding_dim, activation='linear')(encoder[2])\n",
    "\n",
    "    # decoder\n",
    "    decoder = create_layers(encoded, 15)\n",
    "    decoder_skip = create_layers(decoder[2], 15)\n",
    "    decoder = create_layers(decoder_skip[2] + decoder[1], 15)\n",
    "    decoder = create_layers(decoder[2], 15)\n",
    "    decoder = create_layers(decoder[2], 25)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoder[2])\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    autoencoder.compile(optimizer=adam, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "input_dim = base_cadastral_scaled.shape[1]\n",
    "encoding_dim = 3\n",
    "\n",
    "autoencoder = autoencoder_model(input_dim, encoding_dim)\n",
    "histories = []\n",
    "history = autoencoder.fit(base_cadastral_scaled, base_cadastral_scaled, epochs=100, batch_size=64, validation_split=0.2)\n",
    "histories.append(history)\n",
    "\n",
    "encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-18].output)\n",
    "encoded_data = encoder.predict(base_cadastral_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualização do treinamento do modelo\n",
    "\n",
    "# plot losses\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = fig.add_subplot(4, 4, i+1)\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## análise de erro do modelo\n",
    "\n",
    "decoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[-1].output)\n",
    "decoded_data = decoder.predict(base_cadastral_scaled)\n",
    "\n",
    "decoded_data = pd.DataFrame(decoded_data, columns=base_cadastral_scaled.columns)\n",
    "decoded_data['error'] = np.reshape(np.mean(np.abs(base_cadastral_scaled.values - decoded_data.values), axis=1), (-1, 1))\n",
    "\n",
    "decoded_data['error'].hist(bins=100)\n",
    "plt.show()\n",
    "\n",
    "col = 'segmento_industrial'\n",
    "base_cadastral_raw_drop = base_cadastral_date.drop(['ddd', 'cep_2_dig'], axis=1)\n",
    "display(base_cadastral_raw_drop[decoded_data['error'] > 0.6].groupby('segmento_industrial').count() / base_cadastral_raw_drop.groupby('segmento_industrial').count())\n",
    "display(base_cadastral_raw_drop[decoded_data['error'] > 0.6].groupby('dominio_email').count() / base_cadastral_raw_drop.groupby('dominio_email').count())\n",
    "display(base_cadastral_raw_drop[decoded_data['error'] > 0.6].groupby('flag_pf').count() / base_cadastral_raw_drop.groupby('flag_pf').count())\n",
    "display(base_cadastral_raw_drop[decoded_data['error'] > 0.6].groupby('porte').count() / base_cadastral_raw_drop.groupby('porte').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualização do cliente em 3 dimensões\n",
    "\n",
    "# check uniqueness of each encoded representation\n",
    "encoded_data_df = pd.DataFrame(encoded_data, columns=['encoded_1', 'encoded_2', 'encoded_3'])\n",
    "if encoded_data_df.groupby(['encoded_1', 'encoded_2', 'encoded_3']).size().shape[0] / encoded_data_df.shape[0]:\n",
    "    print('All encoded representations are unique.')\n",
    "else:\n",
    "    print('There are repeated encoded representations.')\n",
    "    print(encoded_data_df\n",
    "            .groupby(['encoded_1', 'encoded_2', 'encoded_3'])\n",
    "            .size()\n",
    "            .reset_index()\n",
    "            .rename(columns={0:'count'})\n",
    "            .sort_values(by='count', ascending=False))\n",
    "\n",
    "\n",
    "# check if the encoded data is separable unsing 16 plots\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, col in enumerate(base_cadastral_scaled.columns):\n",
    "    ax = fig.add_subplot(4, 4, i+1, projection='3d')\n",
    "    ax.scatter(encoded_data_df['encoded_1'], encoded_data_df['encoded_2'], encoded_data_df['encoded_3'], c=base_cadastral_scaled[col], cmap='coolwarm')\n",
    "    plt.title(col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Each encoded client and its id:')\n",
    "encoded_data_df['id_cliente'] = base_cadastral['id_cliente']\n",
    "encoded_data_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparo para Modelagem\n",
    "- junção dos dados e feature engineering.\n",
    "- testagem de modelos de prateleira --> regressão logistica tem melhor resultado.\n",
    "- modelo de deteção de anomalia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Junção dos dados e feature engineering\n",
    "\n",
    "# statisticaly significant data is lost in the followin merges\n",
    "base_info_join = base_info_date.merge(encoded_data_df, on='id_cliente', how='inner')\n",
    "base_join = base_info_join.merge(base_pagamentos_date, on=['id_cliente', 'safra_ref'], how='inner')\n",
    "base_join_date_ordered = base_join.sort_values(by=['safra_ref'])\n",
    "\n",
    "def encode_dates_given_ref(df, columns, ref):\n",
    "    df_copy = df.copy()\n",
    "    for col in columns:\n",
    "        min_date = ref\n",
    "        df_copy[col + '_since_min'] = (df_copy[col] - min_date).dt.days + 1\n",
    "        df_copy[col + '_year'] = df_copy[col].dt.year\n",
    "        df_copy[col + '_month'] = df_copy[col].dt.month\n",
    "        df_copy[col + '_day'] = df_copy[col].dt.day\n",
    "        df_copy.drop(col, inplace=True, axis=1)\n",
    "    return df_copy\n",
    "\n",
    "MIN_DATE = base_join_date_ordered['data_emissao_documento'].min()\n",
    "base_join_date = encode_dates_given_ref(base_join_date_ordered, ['data_emissao_documento', 'data_vencimento'], MIN_DATE)\n",
    "\n",
    "# feature engineering\n",
    "base_join_date['renda_por_funcionario'] = base_join_date['renda_mes_anterior'] / base_join_date['no_funcionarios'].apply(lambda x: x if x > 0 else 1)\n",
    "base_join_date['valor_a_pagar_por_renda'] = base_join_date['valor_a_pagar'] / base_join_date['renda_mes_anterior']\n",
    "base_join_date['duracao_emprestimo'] = base_join_date['data_vencimento_since_min'] - base_join_date['data_emissao_documento_since_min']\n",
    "# add number of loans late by client at the time of the loan / total number of loans taken by the client at the time of the loan\n",
    "# add number of frauds by client at the time of the loan / total number of loans taken by the client at the time of the loan\n",
    "\n",
    "X = base_join_date.drop(['id_cliente', 'safra_ref', 'data_pagamento', 'late_payment', 'fraud'], axis=1)\n",
    "y = base_join_date['fraud']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_eval = scaler.transform(X_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testagem de modelos de prateleira --> regressão logistica tem melhor resultado\n",
    "\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Define a list of classifiers to be used\n",
    "classifiers = [\n",
    "    (LogisticRegression(class_weight='balanced'), {\n",
    "        'C': (1e-3, 1e+2, 'log-uniform'),\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    }),\n",
    "    (DecisionTreeClassifier(class_weight='balanced'), {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': (1, 50),\n",
    "        'min_samples_split': (2, 10)\n",
    "    }),\n",
    "    (RandomForestClassifier(n_estimators=10, class_weight='balanced'), {\n",
    "        'max_depth': (1, 50),\n",
    "        'min_samples_split': (2, 10)\n",
    "    }),\n",
    "    (LinearSVC(class_weight='balanced', random_state=42), {\n",
    "        'C': (1e-3, 1e+2, 'log-uniform'),\n",
    "        'loss': ['hinge', 'squared_hinge']\n",
    "    })\n",
    "]\n",
    "\n",
    "# Define a function to train and evaluate the models\n",
    "def train_eval_models(clfs, X_train, y_train, X_eval, y_eval, filename):\n",
    "    accuracy, precision, recall, f1, roc_auc = [], [], [], [], []\n",
    "    best_models = {}\n",
    "    for i, (clf, params) in enumerate(clfs):\n",
    "        clf_name = clf.__class__.__name__\n",
    "\n",
    "        # Define the BayesSearchCV\n",
    "        opt = BayesSearchCV(clf, params, n_iter=25, cv=3, n_jobs=-1) # change n_iter to 50 does not improve the results significantly\n",
    "\n",
    "        # Train the classifier\n",
    "        opt.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the eval set\n",
    "        y_pred = opt.predict(X_eval)\n",
    "\n",
    "        # Save the best model\n",
    "        best_models[clf_name] = opt.best_estimator_\n",
    "        \n",
    "        # Calculate the metrics\n",
    "        accuracy.append(accuracy_score(y_eval, y_pred))\n",
    "        precision.append(precision_score(y_eval, y_pred))\n",
    "        recall.append(recall_score(y_eval, y_pred))\n",
    "        f1.append(f1_score(y_eval, y_pred))\n",
    "        roc_auc.append(roc_auc_score(y_eval, y_pred))\n",
    "\n",
    "        # Print the best parameters and the metrics\n",
    "        print(f'Best parameters for {clf_name}: {opt.best_params_}')\n",
    "        print(f'Accuracy: {accuracy[-1]:.2f}')\n",
    "        print(f'Precision: {precision[-1]:.2f}')\n",
    "        print(f'Recall: {recall[-1]:.2f}')\n",
    "        print(f'F1: {f1[-1]:.2f}')\n",
    "        print(f'ROC AUC: {roc_auc[-1]:.2f}')\n",
    "        print('-----------------------------------------------------')\n",
    "\n",
    "    return best_models\n",
    "\n",
    "# Call the function\n",
    "best_models = train_eval_models(classifiers, X_train, y_train, X_eval, y_eval, filename='results.txt')\n",
    "best_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modelo de deteção de anomalia\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "class IsolationForestWrapper(BaseEstimator):\n",
    "    def __init__(self, n_estimators=10, max_samples='auto', contamination=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.contamination = contamination\n",
    "        self.model = IsolationForest(n_estimators=self.n_estimators, \n",
    "                                     max_samples=self.max_samples, \n",
    "                                     contamination=self.contamination)\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.y_train = (y == 1).astype(int) if y is not None else None\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = self.model.predict(X)\n",
    "        return (pred == 1).astype(int)\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        if y is not None:\n",
    "            y = (y == 1).astype(int)\n",
    "            y_pred = self.predict(X)\n",
    "            return f1_score(y, y_pred)\n",
    "        elif self.y_train is not None:\n",
    "            y_pred = self.predict(X)\n",
    "            return f1_score(self.y_train, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"No y values were given during fitting, cannot calculate score.\")\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        super().set_params(**params)\n",
    "        self.model.set_params(**params)\n",
    "        return self\n",
    "\n",
    "iso_forest = IsolationForestWrapper(n_estimators=30)\n",
    "\n",
    "params = {\n",
    "    'contamination': np.linspace(0.01, 0.2, 20)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(iso_forest, params, n_iter=25, cv=3, n_jobs=-1)\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = opt.predict(X_eval)\n",
    "\n",
    "print(confusion_matrix(y_eval, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_eval, y_pred)\n",
    "precision = precision_score(y_eval, y_pred)\n",
    "recall = recall_score(y_eval, y_pred)\n",
    "f1 = f1_score(y_eval, y_pred)\n",
    "roc_auc = roc_auc_score(y_eval, y_pred)\n",
    "\n",
    "# Print the best parameters and the metrics\n",
    "print(f\"Best parameters for IsolationForest: {opt.best_params_}\")\n",
    "print(f\"Classifier: IsolationForest\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "print('-----------------------------------------------------')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Rede Neural\n",
    "- esse modelo possui o melhor resultado.\n",
    "- o afinamento dos hyperparametros pode ainda deixa-lo melhor.\n",
    "- novas features, como a proporção de fraudes por cliente trariam maior banefício."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.metrics import AUC, Precision, Recall\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)\n",
    "\n",
    "histories = []\n",
    "params_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_layers(input_layer, units, activation='relu', dropout_rate=0.1):\n",
    "    x_dense = Dense(units, activation=activation)(input_layer)\n",
    "    x_bn = BatchNormalization()(x_dense)\n",
    "    x_drop = Dropout(dropout_rate)(x_bn)\n",
    "    return [x_dense, x_bn, x_drop]\n",
    "\n",
    "def fraud_detection_model(input_dim, units=15, learning_rate=0.001, weight_decay=0.0001, loss_function='binary_crossentropy'):\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    # encoder\n",
    "    X = create_layers(input_layer, units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X_skip = create_layers(X[2], units)\n",
    "    X = create_layers(X_skip[2] + X[1], units)\n",
    "    X = create_layers(X[2], units)\n",
    "    X = Dense(1, activation='sigmoid')(X[2])\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=X)\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=adam, loss=loss_function, metrics=[AUC(), Precision(), Recall()])\n",
    "\n",
    "    return model\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# use bayes optimization to find the best combination of lr, wd, class_weight, and loss function\n",
    "params = {\n",
    "    'units': 90,\n",
    "    'lr': 0.002,\n",
    "    'wd': 0.0003,\n",
    "    'lf': 'binary_crossentropy',\n",
    "    'cw': {0:1, 1:4}\n",
    "}\n",
    "\n",
    "model = fraud_detection_model(input_dim=input_dim, units=params['units'], learning_rate=params['lr'], weight_decay=params['wd'], loss_function=params['lf'])\n",
    "history = model.fit(X_train, y, epochs=40, batch_size=512, validation_split=0.2, class_weight=params['cw'])\n",
    "\n",
    "y_pred = model.predict(X_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories.append(history)\n",
    "params_list.append(params)\n",
    "\n",
    "# plot losses\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "for i, history in enumerate(histories):\n",
    "    ax = fig.add_subplot(5, 5, i+1)\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    title = f'u{ params_list[i][\"units\"]}, lr {params_list[i][\"lr\"]:.2e}, wd {params_list[i][\"wd\"]:.2e}, cw {params_list[i][\"cw\"][1]}'\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "- usa o modelo anterior para obter a probabilidade de inadimplencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Junção dos dados e feature engineering\n",
    "\n",
    "# statisticaly significant data is lost in the followin merges\n",
    "base_info_join = base_info_date.merge(encoded_data_df, on='id_cliente', how='inner')\n",
    "base_join_test = base_info_join.merge(base_pagamentos_teste_date, on=['id_cliente', 'safra_ref'], how='inner')\n",
    "base_join_test_date_ordered = base_join_test.sort_values(by=['safra_ref'])\n",
    "\n",
    "base_join_date_test = encode_dates_given_ref(base_join_test_date_ordered, ['data_emissao_documento', 'data_vencimento'], MIN_DATE)\n",
    "\n",
    "# feature engineering\n",
    "base_join_date_test['renda_por_funcionario'] = base_join_date_test['renda_mes_anterior'] / base_join_date_test['no_funcionarios'].apply(lambda x: x if x > 0 else 1)\n",
    "base_join_date_test['valor_a_pagar_por_renda'] = base_join_date_test['valor_a_pagar'] / base_join_date_test['renda_mes_anterior']\n",
    "base_join_date_test['duracao_emprestimo'] = base_join_date_test['data_vencimento_since_min'] - base_join_date_test['data_emissao_documento_since_min']\n",
    "\n",
    "X_test = base_join_date_test.drop(['id_cliente', 'safra_ref'], axis=1)\n",
    "\n",
    "# scale the data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred.columns = ['INADIMPLENTE']\n",
    "data_and_pred = pd.concat([base_join_date_test, y_pred], axis=1)\n",
    "\n",
    "final = data_and_pred[['id_cliente', 'safra_ref', 'INADIMPLENTE']]\n",
    "final.columns = final.columns.str.upper()\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
